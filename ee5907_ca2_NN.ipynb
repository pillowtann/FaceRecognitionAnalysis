{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 00. Load Data\n",
    "##### Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "# for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_list(num_items, start, end, set_seed=123):\n",
    "    random.seed(set_seed)\n",
    "    selected_idx = [i for i in range(start,end)]\n",
    "    random.shuffle(selected_idx)\n",
    "    return sorted(selected_idx[0:num_items])\n",
    "\n",
    "def train_test_split(input_list, train_proportion=0.7, set_seed=123):\n",
    "    train_idx = get_random_list(num_items=round(len(input_list)*train_proportion), \n",
    "                                start=0, end=len(input_list), set_seed=123)\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for i in range(0,len(input_list)):\n",
    "        if i in train_idx:\n",
    "            train_list.append(input_list[i])\n",
    "        else:\n",
    "            test_list.append(input_list[i])\n",
    "    return train_list, test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected subjects: [2, 8, 13, 14, 15, 17, 19, 20, 23, 24, 27, 31, 32, 33, 34, 40, 42, 44, 46, 48, 50, 52, 59, 63, 65]\n"
     ]
    }
   ],
   "source": [
    "# choose 25 PIE subjects\n",
    "selected_idx = get_random_list(num_items=25, start=1, end=68, set_seed=123)\n",
    "print('Selected subjects:', selected_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected PIE images: 4250\n",
      "Number of selected selfies: 10\n",
      "Number of selected images: 4260\n"
     ]
    }
   ],
   "source": [
    "NUM_IMG_PER_SUBJ = 170\n",
    "NUM_SELFIES = 10\n",
    "\n",
    "# list of paths to PIE images\n",
    "pie_list = []\n",
    "for subj_idx in selected_idx:\n",
    "    temp_list = ['PIE//'+str(subj_idx)+'//'+str(i+1)+'.jpg' for i in range(0,NUM_IMG_PER_SUBJ)]\n",
    "    pie_list.extend(temp_list)\n",
    "\n",
    "# list of paths to selfies\n",
    "selfies_list = ['selfies//formatted//'+str(i+1)+'.jpg' for i in range(0,NUM_SELFIES)]\n",
    "\n",
    "# list of paths to all images of interest\n",
    "list_of_img_end_paths = pie_list + selfies_list\n",
    "\n",
    "print('Number of selected PIE images:', len(pie_list))\n",
    "print('Number of selected selfies:', len(selfies_list))\n",
    "print('Number of selected images:', len(list_of_img_end_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number||Proportion of train PIE images: 2975 || 0.7\n",
      "Number||Proportion of test PIE images: 1275 || 0.3\n",
      "Number||Proportion of train selfies: 7 || 0.7\n",
      "Number||Proportion of test selfies: 3 || 0.3\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PROPORTION = 0.7\n",
    "\n",
    "# split PIE train and test\n",
    "pie_train_list, pie_test_list = train_test_split(pie_list, train_proportion=TRAIN_PROPORTION, set_seed=123)\n",
    "\n",
    "# split selfies train and test\n",
    "selfies_train_list, selfies_test_list = train_test_split(selfies_list, train_proportion=TRAIN_PROPORTION, set_seed=123)\n",
    "\n",
    "print('Number||Proportion of train PIE images:', len(pie_train_list), '||', len(pie_train_list)/len(pie_list))\n",
    "print('Number||Proportion of test PIE images:', len(pie_test_list), '||', len(pie_test_list)/len(pie_list))\n",
    "print('Number||Proportion of train selfies:', len(selfies_train_list), '||', len(selfies_train_list)/len(selfies_list))\n",
    "print('Number||Proportion of test selfies:', len(selfies_test_list), '||', len(selfies_test_list)/len(selfies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_path_list(list_of_img_paths, label_type='general'):\n",
    "    label_list = []\n",
    "    for path in list_of_img_paths:\n",
    "        splitted = path.split('//')\n",
    "        if splitted[0]=='PIE':\n",
    "            if label_type=='general':\n",
    "                label_list.append(splitted[0])\n",
    "            elif label_type=='specific':\n",
    "                label_list.append(splitted[1])\n",
    "            else:\n",
    "                print(\"Please specify label_type as 'general' or 'specific'\")\n",
    "        else:\n",
    "            if label_type=='general':\n",
    "                label_list.append(splitted[0])\n",
    "            elif label_type=='specific':\n",
    "                label_list.append('selfies')\n",
    "            else:\n",
    "                print(\"Please specify label_type as 'general' or 'specific'\")\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_vectorise_images(list_of_img_paths):\n",
    "    # read image\n",
    "    path = os.path.abspath('')\n",
    "\n",
    "    for counter, end_paths in enumerate(list_of_img_paths):\n",
    "        # load image\n",
    "        img = cv2.imread(os.path.join(path, end_paths))\n",
    "\n",
    "        # vectorise image\n",
    "        array_size = img.shape[0]*img.shape[1]\n",
    "        vectorised_img = img.copy()\n",
    "        vectorised_img = vectorised_img.reshape(array_size,3)\n",
    "        vectorised_img = np.array([i[0] for i in vectorised_img], dtype=int)\n",
    "\n",
    "        # add to dataset\n",
    "        if counter == 0:\n",
    "            data = vectorised_img.reshape(1,array_size).copy()\n",
    "        else:\n",
    "            data = np.concatenate((data, vectorised_img.reshape(1,array_size)), axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorised and loaded train data: (2982, 1024)\n",
      "Vectorised and loaded test data: (1278, 1024)\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "train_data = load_and_vectorise_images(pie_train_list+selfies_train_list)\n",
    "test_data = load_and_vectorise_images(pie_test_list+selfies_test_list)\n",
    "print('Vectorised and loaded train data:', train_data.shape)\n",
    "print('Vectorised and loaded test data:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare specific labels (target var for classification task)\n",
    "train_labels = get_labels_from_path_list(pie_train_list+selfies_train_list, label_type='specific')\n",
    "test_labels = get_labels_from_path_list(pie_test_list+selfies_test_list, label_type='specific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format labels into numeric values (& storing conversion dictionary)\n",
    "def convert_str_to_num(str_list, str_to_num=None):\n",
    "    unique = list(set(str_list))\n",
    "    if str_to_num is None:\n",
    "        str_to_num = {string:idx for idx, string in enumerate(unique)}\n",
    "    else:\n",
    "        missing = [i for i in unique if i not in str_to_num.keys()]\n",
    "        max_idx = max(str_to_num.values())\n",
    "        for string in missing:\n",
    "            str_to_num[string]=max_idx\n",
    "            max_idx=+1\n",
    "        \n",
    "    num_list = [str_to_num[string] for string in str_list]\n",
    "    return num_list, str_to_num\n",
    "\n",
    "train_labels_num, train_str_to_num = convert_str_to_num(train_labels)\n",
    "test_labels_num, str_to_num = convert_str_to_num(test_labels, train_str_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if false means we have unseen labels in test set, which might be problematic\n",
    "assert train_str_to_num == str_to_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05. Neural Network (NN)\n",
    "Train a CNN with two convolutional\n",
    "layers and one fully connected layer, with the architecture specified as follows: number of\n",
    "nodes: 20-50-500-21. The number of the nodes in the last layer is fixed as 21 as we are performing\n",
    "21-category (20 CMU PIE faces plus 1 for yourself) classification. Convolutional\n",
    "kernel sizes are set as 5. Each convolutional layer is followed by a max pooling layer with\n",
    "a kernel size of 2 and stride of 2. The fully connected layer is followed by ReLU. Train the\n",
    "network and report the final classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NN for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 1e-2\n",
    "GENERATOR_PARAMS = {\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0,\n",
    "    'drop_last': True, #ignore last incomplete batch\n",
    "    'pin_memory': True\n",
    "}\n",
    "\n",
    "# preferences\n",
    "PRINT_ROUND = 50\n",
    "\n",
    "# gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    # print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data into pytorch tensor format\n",
    "def prepare_sequence(seq):\n",
    "    return torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "class FormatDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # Run multiple rows at once, i.e. reduce enumerate\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        feats_in = prepare_sequence(self.features[index])\n",
    "        target_out = prepare_sequence(self.labels[index])\n",
    "\n",
    "        return feats_in, target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 5.182, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: 3.228, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: 3.281, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: 2.619, Accuracy: 10.0%\n",
      "Step 200 | Training Loss: 2.963, Accuracy: 20.0%\n",
      "Step 250 | Training Loss: 1.387, Accuracy: 60.0%\n",
      "Average Accuracy: 20.436%\n",
      "Epoch 1 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 2.043, Accuracy: 50.0%\n",
      "Step 50 | Training Loss: 0.859, Accuracy: 80.0%\n",
      "Step 100 | Training Loss: 1.023, Accuracy: 80.0%\n",
      "Step 150 | Training Loss: 0.194, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.589, Accuracy: 90.0%\n",
      "Step 250 | Training Loss: 0.4, Accuracy: 80.0%\n",
      "Average Accuracy: 79.732%\n",
      "Epoch 2 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.076, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.498, Accuracy: 80.0%\n",
      "Step 100 | Training Loss: 0.172, Accuracy: 90.0%\n",
      "Step 150 | Training Loss: 0.206, Accuracy: 90.0%\n",
      "Step 200 | Training Loss: 0.108, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.026, Accuracy: 100.0%\n",
      "Average Accuracy: 94.128%\n",
      "Epoch 3 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.077, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.01, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.259, Accuracy: 90.0%\n",
      "Step 150 | Training Loss: 0.024, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.003, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.022, Accuracy: 100.0%\n",
      "Average Accuracy: 97.685%\n",
      "Epoch 4 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.021, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.017, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.007, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.145, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.004, Accuracy: 100.0%\n",
      "Average Accuracy: 98.423%\n",
      "Epoch 5 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.012, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.056, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 99.396%\n",
      "Epoch 6 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.004, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 7 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 8 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 9 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 10 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 11 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 12 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 13 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 14 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "--------------------------------------------------------------------------------\n",
      "Testing Accuracy: 97.34%\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim, kernel_size, pool_k_size, pool_stride):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(embed_dim, hidden_dim1, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim1, hidden_dim2, kernel_size=kernel_size)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_k_size, stride=pool_stride)\n",
    "        self.fc1 = nn.Linear(hidden_dim2*5*5, hidden_dim3)\n",
    "        self.fc2 = nn.Linear(hidden_dim3, output_dim)\n",
    "        \n",
    "    def forward(self, feats_in):\n",
    "        batch_size, e_, e_, s_ = feats_in.shape\n",
    "        x = self.pool(F.relu(self.conv1(feats_in.float())))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x),dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# load data\n",
    "training_set = FormatDataset(train_data, np.asarray(train_labels_num))\n",
    "training_generator = DataLoader(training_set, batch_size=BATCH_SIZE, **GENERATOR_PARAMS)\n",
    "\n",
    "test_batch_size = test_data.shape[0]\n",
    "test_set = FormatDataset(test_data, np.asarray(test_labels_num))\n",
    "testing_generator = DataLoader(test_set, batch_size=test_batch_size, **GENERATOR_PARAMS)\n",
    "\n",
    "# load model\n",
    "model = CNN(embed_dim=1,\n",
    "            hidden_dim1=20, \n",
    "            hidden_dim2=50,\n",
    "            hidden_dim3=500,\n",
    "            output_dim=26, \n",
    "            kernel_size=(5,5),\n",
    "            pool_k_size=(2,2),\n",
    "            pool_stride=2\n",
    "           ).to(device)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "results_dict = {}\n",
    "early_stopping = False\n",
    "\n",
    "# train\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Epoch {}'.format(epoch), '-'*80)\n",
    "    results = {'train_loss': [], 'train_acc': []}\n",
    "    for idx, (feats_in, target_out) in enumerate(training_generator):\n",
    "        feats_in = feats_in.to(device).view(BATCH_SIZE, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/len(target_out)\n",
    "        loss = loss_function(tag_scores, target_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        results['train_loss'].append(loss.data.item())\n",
    "        results['train_acc'].append(accuracy)\n",
    "\n",
    "        if idx%PRINT_ROUND==0:\n",
    "            print('Step {} | Training Loss: {}, Accuracy: {}%'.format(idx, round(loss.data.item(),3), round(accuracy*100,3)))\n",
    "\n",
    "    results_dict[epoch] = results\n",
    "    avg_acc = sum(results['train_acc'])/(idx+1)\n",
    "    print('Average Accuracy: {}%'.format(round(avg_acc*100,3)))\n",
    "    \n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    print('-'*80)\n",
    "    for idx, (feats_in, target_out) in enumerate(testing_generator):\n",
    "        feats_in = feats_in.to(device).view(test_batch_size, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/test_batch_size\n",
    "        print('Testing Accuracy: {}%'.format(round(accuracy*100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NN for ALL test set: 97.34%\n",
      "Accuracy of NN for PIE test set: 97.333%\n",
      "Accuracy of NN for selfies test set: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# selfies is numbered as 25\n",
    "correct_selfies = sum([1 for pred,act in zip(predicted,target_out) if (pred==act) & (act==25)])\n",
    "correct_PIE = correct - correct_selfies\n",
    "total_selfies = 3\n",
    "total_PIE = test_batch_size-total_selfies\n",
    "print('Accuracy of NN for ALL test set: {}%'.format(round(accuracy*100,3)))\n",
    "print('Accuracy of NN for PIE test set: {}%'.format(round((correct_PIE/total_PIE)*100,3)))\n",
    "print('Accuracy of NN for selfies test set: {}%'.format(round((correct_selfies/total_selfies)*100,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimenting with different network architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 11.669, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 0.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 1 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 10.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 2 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 0.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 3 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 0.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 4 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 0.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 5 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 0.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 6 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 0.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 7 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 20.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 0.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 8 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 10.0%\n",
      "Average Accuracy: 3.758%\n",
      "Epoch 9 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 10.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 10 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 10.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 11 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 0.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 12 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 0.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 13 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 0.0%\n",
      "Average Accuracy: 3.792%\n",
      "Epoch 14 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 50 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: nan, Accuracy: 10.0%\n",
      "Step 200 | Training Loss: nan, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: nan, Accuracy: 10.0%\n",
      "Average Accuracy: 3.792%\n",
      "--------------------------------------------------------------------------------\n",
      "Testing Accuracy: 4.46%\n",
      "Accuracy of NN for ALL test set: 4.46%\n",
      "Accuracy of NN for PIE test set: 4.471%\n",
      "Accuracy of NN for selfies test set: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# 1. Without ReLU between CNN Layers (failed model)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim, kernel_size, pool_k_size, pool_stride):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(embed_dim, hidden_dim1, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim1, hidden_dim2, kernel_size=kernel_size)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_k_size, stride=pool_stride)\n",
    "        self.fc1 = nn.Linear(hidden_dim2*5*5, hidden_dim3)\n",
    "        self.fc2 = nn.Linear(hidden_dim3, output_dim)\n",
    "        \n",
    "    def forward(self, feats_in):\n",
    "        batch_size, e_, e_, s_ = feats_in.shape\n",
    "        x = self.pool(self.conv1(feats_in.float()))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x),dim=1)\n",
    "        return x\n",
    "\n",
    "# load data\n",
    "training_set = FormatDataset(train_data, np.asarray(train_labels_num))\n",
    "training_generator = DataLoader(training_set, batch_size=BATCH_SIZE, **GENERATOR_PARAMS)\n",
    "\n",
    "test_batch_size = test_data.shape[0]\n",
    "test_set = FormatDataset(test_data, np.asarray(test_labels_num))\n",
    "testing_generator = DataLoader(test_set, batch_size=test_batch_size, **GENERATOR_PARAMS)\n",
    "\n",
    "# load model\n",
    "model = CNN(embed_dim=1,\n",
    "            hidden_dim1=20, \n",
    "            hidden_dim2=50,\n",
    "            hidden_dim3=500,\n",
    "            output_dim=26, \n",
    "            kernel_size=(5,5),\n",
    "            pool_k_size=(2,2),\n",
    "            pool_stride=2\n",
    "           ).to(device)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "results_dict = {}\n",
    "early_stopping = False\n",
    "\n",
    "# train\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Epoch {}'.format(epoch), '-'*80)\n",
    "    results = {'train_loss': [], 'train_acc': []}\n",
    "    for idx, (feats_in, target_out) in enumerate(training_generator):\n",
    "        feats_in = feats_in.to(device).view(BATCH_SIZE, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/len(target_out)\n",
    "        loss = loss_function(tag_scores, target_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        results['train_loss'].append(loss.data.item())\n",
    "        results['train_acc'].append(accuracy)\n",
    "\n",
    "        if idx%PRINT_ROUND==0:\n",
    "            print('Step {} | Training Loss: {}, Accuracy: {}%'.format(idx, round(loss.data.item(),3), round(accuracy*100,3)))\n",
    "\n",
    "    results_dict[epoch] = results\n",
    "    avg_acc = sum(results['train_acc'])/(idx+1)\n",
    "    print('Average Accuracy: {}%'.format(round(avg_acc*100,3)))\n",
    "\n",
    "# load data\n",
    "training_generator = DataLoader(training_set, batch_size=BATCH_SIZE, **GENERATOR_PARAMS)\n",
    "\n",
    "test_batch_size = test_data.shape[0]\n",
    "test_set = FormatDataset(test_data, np.asarray(test_labels_num))\n",
    "testing_generator = DataLoader(test_set, batch_size=test_batch_size, **GENERATOR_PARAMS)\n",
    "\n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    print('-'*80)\n",
    "    for idx, (feats_in, target_out) in enumerate(testing_generator):\n",
    "        feats_in = feats_in.to(device).view(test_batch_size, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/test_batch_size\n",
    "        print('Testing Accuracy: {}%'.format(round(accuracy*100,3)))\n",
    "        \n",
    "# selfies is numbered as 25\n",
    "correct_selfies = sum([1 for pred,act in zip(predicted,target_out) if (pred==act) & (act==25)])\n",
    "correct_PIE = correct - correct_selfies\n",
    "total_selfies = 3\n",
    "total_PIE = test_batch_size-total_selfies\n",
    "print('Accuracy of NN for ALL test set: {}%'.format(round(accuracy*100,3)))\n",
    "print('Accuracy of NN for PIE test set: {}%'.format(round((correct_PIE/total_PIE)*100,3)))\n",
    "print('Accuracy of NN for selfies test set: {}%'.format(round((correct_selfies/total_selfies)*100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 15.822, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: 3.286, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: 3.351, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: 3.267, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: 3.248, Accuracy: 20.0%\n",
      "Step 250 | Training Loss: 3.303, Accuracy: 0.0%\n",
      "Step 300 | Training Loss: 3.248, Accuracy: 0.0%\n",
      "Step 350 | Training Loss: 3.27, Accuracy: 0.0%\n",
      "Step 400 | Training Loss: 3.237, Accuracy: 0.0%\n",
      "Step 450 | Training Loss: 3.266, Accuracy: 0.0%\n",
      "Step 500 | Training Loss: 3.267, Accuracy: 20.0%\n",
      "Step 550 | Training Loss: 3.251, Accuracy: 0.0%\n",
      "Average Accuracy: 3.893%\n",
      "Epoch 1 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 3.242, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: 3.239, Accuracy: 20.0%\n",
      "Step 100 | Training Loss: 3.245, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: 3.253, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: 3.242, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: 3.237, Accuracy: 0.0%\n",
      "Step 300 | Training Loss: 3.241, Accuracy: 0.0%\n",
      "Step 350 | Training Loss: 3.256, Accuracy: 0.0%\n",
      "Step 400 | Training Loss: 3.552, Accuracy: 0.0%\n",
      "Step 450 | Training Loss: 3.238, Accuracy: 20.0%\n",
      "Step 500 | Training Loss: 3.251, Accuracy: 0.0%\n",
      "Step 550 | Training Loss: 3.456, Accuracy: 0.0%\n",
      "Average Accuracy: 5.067%\n",
      "Epoch 2 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 3.002, Accuracy: 20.0%\n",
      "Step 50 | Training Loss: 3.225, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: 3.075, Accuracy: 20.0%\n",
      "Step 150 | Training Loss: 3.361, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: 2.71, Accuracy: 20.0%\n",
      "Step 250 | Training Loss: 2.36, Accuracy: 40.0%\n",
      "Step 300 | Training Loss: 3.132, Accuracy: 20.0%\n",
      "Step 350 | Training Loss: 3.21, Accuracy: 0.0%\n",
      "Step 400 | Training Loss: 2.533, Accuracy: 40.0%\n",
      "Step 450 | Training Loss: 3.715, Accuracy: 0.0%\n",
      "Step 500 | Training Loss: 3.333, Accuracy: 20.0%\n",
      "Step 550 | Training Loss: 3.152, Accuracy: 20.0%\n",
      "Average Accuracy: 9.832%\n",
      "Epoch 3 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 2.877, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: 3.206, Accuracy: 20.0%\n",
      "Step 100 | Training Loss: 2.485, Accuracy: 20.0%\n",
      "Step 150 | Training Loss: 2.734, Accuracy: 40.0%\n",
      "Step 200 | Training Loss: 2.812, Accuracy: 40.0%\n",
      "Step 250 | Training Loss: 2.668, Accuracy: 20.0%\n",
      "Step 300 | Training Loss: 3.14, Accuracy: 0.0%\n",
      "Step 350 | Training Loss: 2.927, Accuracy: 20.0%\n",
      "Step 400 | Training Loss: 1.85, Accuracy: 60.0%\n",
      "Step 450 | Training Loss: 1.564, Accuracy: 60.0%\n",
      "Step 500 | Training Loss: 2.056, Accuracy: 20.0%\n",
      "Step 550 | Training Loss: 1.655, Accuracy: 80.0%\n",
      "Average Accuracy: 20.034%\n",
      "Epoch 4 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 2.543, Accuracy: 40.0%\n",
      "Step 50 | Training Loss: 2.788, Accuracy: 20.0%\n",
      "Step 100 | Training Loss: 3.378, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: 3.105, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: 3.075, Accuracy: 20.0%\n",
      "Step 250 | Training Loss: 3.201, Accuracy: 0.0%\n",
      "Step 300 | Training Loss: 3.182, Accuracy: 0.0%\n",
      "Step 350 | Training Loss: 2.871, Accuracy: 0.0%\n",
      "Step 400 | Training Loss: 1.963, Accuracy: 40.0%\n",
      "Step 450 | Training Loss: 3.23, Accuracy: 0.0%\n",
      "Step 500 | Training Loss: 3.275, Accuracy: 0.0%\n",
      "Step 550 | Training Loss: 3.223, Accuracy: 0.0%\n",
      "Average Accuracy: 9.899%\n",
      "Epoch 5 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 2.688, Accuracy: 20.0%\n",
      "Step 50 | Training Loss: 2.58, Accuracy: 40.0%\n",
      "Step 100 | Training Loss: 2.508, Accuracy: 20.0%\n",
      "Step 150 | Training Loss: 3.167, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: 3.189, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: 3.038, Accuracy: 0.0%\n",
      "Step 300 | Training Loss: 2.487, Accuracy: 40.0%\n",
      "Step 350 | Training Loss: 2.808, Accuracy: 0.0%\n",
      "Step 400 | Training Loss: 3.131, Accuracy: 0.0%\n",
      "Step 450 | Training Loss: 1.695, Accuracy: 40.0%\n",
      "Step 500 | Training Loss: 3.198, Accuracy: 0.0%\n",
      "Step 550 | Training Loss: 2.515, Accuracy: 20.0%\n",
      "Average Accuracy: 20.168%\n",
      "Epoch 6 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 2.34, Accuracy: 20.0%\n",
      "Step 50 | Training Loss: 2.439, Accuracy: 40.0%\n",
      "Step 100 | Training Loss: 3.343, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: 2.68, Accuracy: 20.0%\n",
      "Step 200 | Training Loss: 3.869, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: 2.5, Accuracy: 40.0%\n",
      "Step 300 | Training Loss: 1.505, Accuracy: 40.0%\n",
      "Step 350 | Training Loss: 3.063, Accuracy: 20.0%\n",
      "Step 400 | Training Loss: 2.727, Accuracy: 40.0%\n",
      "Step 450 | Training Loss: 1.262, Accuracy: 80.0%\n",
      "Step 500 | Training Loss: 1.664, Accuracy: 40.0%\n",
      "Step 550 | Training Loss: 2.615, Accuracy: 20.0%\n",
      "Average Accuracy: 34.295%\n",
      "Epoch 7 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 2.451, Accuracy: 20.0%\n",
      "Step 50 | Training Loss: 1.272, Accuracy: 40.0%\n",
      "Step 100 | Training Loss: 1.637, Accuracy: 40.0%\n",
      "Step 150 | Training Loss: 0.663, Accuracy: 80.0%\n",
      "Step 200 | Training Loss: 0.886, Accuracy: 80.0%\n",
      "Step 250 | Training Loss: 0.932, Accuracy: 80.0%\n",
      "Step 300 | Training Loss: 1.493, Accuracy: 80.0%\n",
      "Step 350 | Training Loss: 0.875, Accuracy: 80.0%\n",
      "Step 400 | Training Loss: 1.81, Accuracy: 40.0%\n",
      "Step 450 | Training Loss: 1.527, Accuracy: 40.0%\n",
      "Step 500 | Training Loss: 1.475, Accuracy: 60.0%\n",
      "Step 550 | Training Loss: 3.094, Accuracy: 0.0%\n",
      "Average Accuracy: 50.336%\n",
      "Epoch 8 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 3.113, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: 3.05, Accuracy: 20.0%\n",
      "Step 100 | Training Loss: 3.269, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: 3.207, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: 3.318, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: 3.169, Accuracy: 0.0%\n",
      "Step 300 | Training Loss: 3.127, Accuracy: 0.0%\n",
      "Step 350 | Training Loss: 3.299, Accuracy: 0.0%\n",
      "Step 400 | Training Loss: 3.163, Accuracy: 0.0%\n",
      "Step 450 | Training Loss: 3.082, Accuracy: 20.0%\n",
      "Step 500 | Training Loss: 2.873, Accuracy: 20.0%\n",
      "Step 550 | Training Loss: 2.04, Accuracy: 20.0%\n",
      "Average Accuracy: 10.403%\n",
      "Epoch 9 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 1.434, Accuracy: 80.0%\n",
      "Step 50 | Training Loss: 1.669, Accuracy: 60.0%\n",
      "Step 100 | Training Loss: 2.109, Accuracy: 40.0%\n",
      "Step 150 | Training Loss: 3.586, Accuracy: 20.0%\n",
      "Step 200 | Training Loss: 1.438, Accuracy: 60.0%\n",
      "Step 250 | Training Loss: 0.47, Accuracy: 80.0%\n",
      "Step 300 | Training Loss: 1.598, Accuracy: 60.0%\n",
      "Step 350 | Training Loss: 3.07, Accuracy: 40.0%\n",
      "Step 400 | Training Loss: 1.519, Accuracy: 40.0%\n",
      "Step 450 | Training Loss: 0.969, Accuracy: 80.0%\n",
      "Step 500 | Training Loss: 3.365, Accuracy: 40.0%\n",
      "Step 550 | Training Loss: 0.05, Accuracy: 100.0%\n",
      "Average Accuracy: 56.879%\n",
      "Epoch 10 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 3.598, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: 3.172, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: 3.261, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: 3.137, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: 3.218, Accuracy: 0.0%\n",
      "Step 250 | Training Loss: 3.051, Accuracy: 0.0%\n",
      "Step 300 | Training Loss: 3.246, Accuracy: 20.0%\n",
      "Step 350 | Training Loss: 3.155, Accuracy: 20.0%\n",
      "Step 400 | Training Loss: 3.251, Accuracy: 0.0%\n",
      "Step 450 | Training Loss: 3.258, Accuracy: 0.0%\n",
      "Step 500 | Training Loss: 3.256, Accuracy: 0.0%\n",
      "Step 550 | Training Loss: 2.6, Accuracy: 60.0%\n",
      "Average Accuracy: 8.389%\n",
      "Epoch 11 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 2.262, Accuracy: 20.0%\n",
      "Step 50 | Training Loss: 0.507, Accuracy: 80.0%\n",
      "Step 100 | Training Loss: 1.154, Accuracy: 60.0%\n",
      "Step 150 | Training Loss: 2.275, Accuracy: 40.0%\n",
      "Step 200 | Training Loss: 1.822, Accuracy: 40.0%\n",
      "Step 250 | Training Loss: 1.652, Accuracy: 60.0%\n",
      "Step 300 | Training Loss: 3.119, Accuracy: 0.0%\n",
      "Step 350 | Training Loss: 2.584, Accuracy: 20.0%\n",
      "Step 400 | Training Loss: 1.819, Accuracy: 20.0%\n",
      "Step 450 | Training Loss: 1.881, Accuracy: 60.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 | Training Loss: 1.341, Accuracy: 60.0%\n",
      "Step 550 | Training Loss: 0.595, Accuracy: 80.0%\n",
      "Average Accuracy: 57.248%\n",
      "Epoch 12 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 2.059, Accuracy: 20.0%\n",
      "Step 50 | Training Loss: 0.346, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.404, Accuracy: 80.0%\n",
      "Step 150 | Training Loss: 0.01, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 2.483, Accuracy: 20.0%\n",
      "Step 250 | Training Loss: 1.584, Accuracy: 60.0%\n",
      "Step 300 | Training Loss: 1.63, Accuracy: 60.0%\n",
      "Step 350 | Training Loss: 3.126, Accuracy: 0.0%\n",
      "Step 400 | Training Loss: 3.174, Accuracy: 0.0%\n",
      "Step 450 | Training Loss: 2.658, Accuracy: 20.0%\n",
      "Step 500 | Training Loss: 3.099, Accuracy: 0.0%\n",
      "Step 550 | Training Loss: 2.699, Accuracy: 20.0%\n",
      "Average Accuracy: 37.517%\n",
      "Epoch 13 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 1.928, Accuracy: 60.0%\n",
      "Step 50 | Training Loss: 2.543, Accuracy: 20.0%\n",
      "Step 100 | Training Loss: 3.402, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: 1.929, Accuracy: 40.0%\n",
      "Step 200 | Training Loss: 2.257, Accuracy: 40.0%\n",
      "Step 250 | Training Loss: 8.72, Accuracy: 0.0%\n",
      "Step 300 | Training Loss: 0.88, Accuracy: 40.0%\n",
      "Step 350 | Training Loss: 1.573, Accuracy: 80.0%\n",
      "Step 400 | Training Loss: 1.992, Accuracy: 40.0%\n",
      "Step 450 | Training Loss: 0.614, Accuracy: 80.0%\n",
      "Step 500 | Training Loss: 3.239, Accuracy: 0.0%\n",
      "Step 550 | Training Loss: 3.117, Accuracy: 0.0%\n",
      "Average Accuracy: 32.081%\n",
      "Epoch 14 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 3.228, Accuracy: 0.0%\n",
      "Step 50 | Training Loss: 3.324, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: 3.248, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: 3.339, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: 1.652, Accuracy: 60.0%\n",
      "Step 250 | Training Loss: 0.847, Accuracy: 60.0%\n",
      "Step 300 | Training Loss: 1.375, Accuracy: 80.0%\n",
      "Step 350 | Training Loss: 2.454, Accuracy: 20.0%\n",
      "Step 400 | Training Loss: 0.653, Accuracy: 100.0%\n",
      "Step 450 | Training Loss: 3.809, Accuracy: 0.0%\n",
      "Step 500 | Training Loss: 2.402, Accuracy: 40.0%\n",
      "Step 550 | Training Loss: 2.608, Accuracy: 20.0%\n",
      "Average Accuracy: 30.604%\n",
      "--------------------------------------------------------------------------------\n",
      "Testing Accuracy: 61.346%\n"
     ]
    }
   ],
   "source": [
    "# 2. Reducing batch sizes\n",
    "BATCH_SIZE=5\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim, kernel_size, pool_k_size, pool_stride):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(embed_dim, hidden_dim1, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim1, hidden_dim2, kernel_size=kernel_size)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_k_size, stride=pool_stride)\n",
    "        self.fc1 = nn.Linear(hidden_dim2*5*5, hidden_dim3)\n",
    "        self.fc2 = nn.Linear(hidden_dim3, output_dim)\n",
    "        \n",
    "    def forward(self, feats_in):\n",
    "        batch_size, e_, e_, s_ = feats_in.shape\n",
    "        x = self.pool(F.relu(self.conv1(feats_in.float())))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x),dim=1)\n",
    "        return x\n",
    "\n",
    "# load data\n",
    "training_set = FormatDataset(train_data, np.asarray(train_labels_num))\n",
    "training_generator = DataLoader(training_set, batch_size=BATCH_SIZE, **GENERATOR_PARAMS)\n",
    "\n",
    "test_batch_size = test_data.shape[0]\n",
    "test_set = FormatDataset(test_data, np.asarray(test_labels_num))\n",
    "testing_generator = DataLoader(test_set, batch_size=test_batch_size, **GENERATOR_PARAMS)\n",
    "\n",
    "# load model\n",
    "model = CNN(embed_dim=1,\n",
    "            hidden_dim1=20, \n",
    "            hidden_dim2=50,\n",
    "            hidden_dim3=500,\n",
    "            output_dim=26, \n",
    "            kernel_size=(5,5),\n",
    "            pool_k_size=(2,2),\n",
    "            pool_stride=2\n",
    "           ).to(device)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "results_dict = {}\n",
    "early_stopping = False\n",
    "\n",
    "# train\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Epoch {}'.format(epoch), '-'*80)\n",
    "    results = {'train_loss': [], 'train_acc': []}\n",
    "    for idx, (feats_in, target_out) in enumerate(training_generator):\n",
    "        feats_in = feats_in.to(device).view(BATCH_SIZE, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/len(target_out)\n",
    "        loss = loss_function(tag_scores, target_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        results['train_loss'].append(loss.data.item())\n",
    "        results['train_acc'].append(accuracy)\n",
    "\n",
    "        if idx%PRINT_ROUND==0:\n",
    "            print('Step {} | Training Loss: {}, Accuracy: {}%'.format(idx, round(loss.data.item(),3), round(accuracy*100,3)))\n",
    "\n",
    "    results_dict[epoch] = results\n",
    "    avg_acc = sum(results['train_acc'])/(idx+1)\n",
    "    print('Average Accuracy: {}%'.format(round(avg_acc*100,3)))\n",
    "    \n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    print('-'*80)\n",
    "    for idx, (feats_in, target_out) in enumerate(testing_generator):\n",
    "        feats_in = feats_in.to(device).view(test_batch_size, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/test_batch_size\n",
    "        print('Testing Accuracy: {}%'.format(round(accuracy*100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 7.705, Accuracy: 6.0%\n",
      "Step 50 | Training Loss: 2.776, Accuracy: 32.0%\n",
      "Average Accuracy: 15.39%\n",
      "Epoch 1 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 2.81, Accuracy: 18.0%\n",
      "Step 50 | Training Loss: 0.812, Accuracy: 80.0%\n",
      "Average Accuracy: 59.322%\n",
      "Epoch 2 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.486, Accuracy: 88.0%\n",
      "Step 50 | Training Loss: 0.479, Accuracy: 90.0%\n",
      "Average Accuracy: 87.153%\n",
      "Epoch 3 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.221, Accuracy: 94.0%\n",
      "Step 50 | Training Loss: 0.198, Accuracy: 94.0%\n",
      "Average Accuracy: 95.661%\n",
      "Epoch 4 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.085, Accuracy: 98.0%\n",
      "Step 50 | Training Loss: 0.026, Accuracy: 100.0%\n",
      "Average Accuracy: 95.424%\n",
      "Epoch 5 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.68, Accuracy: 92.0%\n",
      "Step 50 | Training Loss: 0.178, Accuracy: 96.0%\n",
      "Average Accuracy: 97.661%\n",
      "Epoch 6 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.022, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.014, Accuracy: 100.0%\n",
      "Average Accuracy: 99.525%\n",
      "Epoch 7 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.025, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.016, Accuracy: 100.0%\n",
      "Average Accuracy: 99.898%\n",
      "Epoch 8 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.006, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.011, Accuracy: 100.0%\n",
      "Average Accuracy: 99.898%\n",
      "Epoch 9 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.004, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.005, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 10 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.005, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 11 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.007, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.005, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 12 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.003, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 13 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.004, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 14 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "--------------------------------------------------------------------------------\n",
      "Testing Accuracy: 97.809%\n"
     ]
    }
   ],
   "source": [
    "# 2. Increasing batch sizes\n",
    "BATCH_SIZE=50\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim, kernel_size, pool_k_size, pool_stride):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(embed_dim, hidden_dim1, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim1, hidden_dim2, kernel_size=kernel_size)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_k_size, stride=pool_stride)\n",
    "        self.fc1 = nn.Linear(hidden_dim2*5*5, hidden_dim3)\n",
    "        self.fc2 = nn.Linear(hidden_dim3, output_dim)\n",
    "        \n",
    "    def forward(self, feats_in):\n",
    "        batch_size, e_, e_, s_ = feats_in.shape\n",
    "        x = self.pool(F.relu(self.conv1(feats_in.float())))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x),dim=1)\n",
    "        return x\n",
    "\n",
    "# load data\n",
    "training_set = FormatDataset(train_data, np.asarray(train_labels_num))\n",
    "training_generator = DataLoader(training_set, batch_size=BATCH_SIZE, **GENERATOR_PARAMS)\n",
    "\n",
    "test_batch_size = test_data.shape[0]\n",
    "test_set = FormatDataset(test_data, np.asarray(test_labels_num))\n",
    "testing_generator = DataLoader(test_set, batch_size=test_batch_size, **GENERATOR_PARAMS)\n",
    "\n",
    "# load model\n",
    "model = CNN(embed_dim=1,\n",
    "            hidden_dim1=20, \n",
    "            hidden_dim2=50,\n",
    "            hidden_dim3=500,\n",
    "            output_dim=26, \n",
    "            kernel_size=(5,5),\n",
    "            pool_k_size=(2,2),\n",
    "            pool_stride=2\n",
    "           ).to(device)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "results_dict = {}\n",
    "early_stopping = False\n",
    "\n",
    "# train\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Epoch {}'.format(epoch), '-'*80)\n",
    "    results = {'train_loss': [], 'train_acc': []}\n",
    "    for idx, (feats_in, target_out) in enumerate(training_generator):\n",
    "        feats_in = feats_in.to(device).view(BATCH_SIZE, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/len(target_out)\n",
    "        loss = loss_function(tag_scores, target_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        results['train_loss'].append(loss.data.item())\n",
    "        results['train_acc'].append(accuracy)\n",
    "\n",
    "        if idx%PRINT_ROUND==0:\n",
    "            print('Step {} | Training Loss: {}, Accuracy: {}%'.format(idx, round(loss.data.item(),3), round(accuracy*100,3)))\n",
    "\n",
    "    results_dict[epoch] = results\n",
    "    avg_acc = sum(results['train_acc'])/(idx+1)\n",
    "    print('Average Accuracy: {}%'.format(round(avg_acc*100,3)))\n",
    "    \n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    print('-'*80)\n",
    "    for idx, (feats_in, target_out) in enumerate(testing_generator):\n",
    "        feats_in = feats_in.to(device).view(test_batch_size, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/test_batch_size\n",
    "        print('Testing Accuracy: {}%'.format(round(accuracy*100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Training Loss: 3.258, Accuracy: 7.031%\n",
      "Epoch 10 | Training Loss: 2.282, Accuracy: 39.844%\n",
      "Epoch 20 | Training Loss: 0.044, Accuracy: 99.219%\n",
      "--------------------------------------------------------------------------------\n",
      "Testing Accuracy: 96.557%\n"
     ]
    }
   ],
   "source": [
    "# 2. Increasing batch sizes\n",
    "BATCH_SIZE=128\n",
    "NUM_EPOCHS=300\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim, kernel_size, pool_k_size, pool_stride):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(embed_dim, hidden_dim1, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim1, hidden_dim2, kernel_size=kernel_size)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_k_size, stride=pool_stride)\n",
    "        self.fc1 = nn.Linear(hidden_dim2*5*5, hidden_dim3)\n",
    "        self.fc2 = nn.Linear(hidden_dim3, output_dim)\n",
    "        \n",
    "    def forward(self, feats_in):\n",
    "        batch_size, e_, e_, s_ = feats_in.shape\n",
    "        x = self.pool(F.relu(self.conv1(feats_in.float())))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x),dim=1)\n",
    "        return x\n",
    "\n",
    "# load data\n",
    "training_set = FormatDataset(train_data, np.asarray(train_labels_num))\n",
    "training_generator = DataLoader(training_set, batch_size=BATCH_SIZE, **GENERATOR_PARAMS)\n",
    "\n",
    "test_batch_size = test_data.shape[0]\n",
    "test_set = FormatDataset(test_data, np.asarray(test_labels_num))\n",
    "testing_generator = DataLoader(test_set, batch_size=test_batch_size, **GENERATOR_PARAMS)\n",
    "\n",
    "# load model\n",
    "model = CNN(embed_dim=1,\n",
    "            hidden_dim1=20, \n",
    "            hidden_dim2=50,\n",
    "            hidden_dim3=500,\n",
    "            output_dim=26, \n",
    "            kernel_size=(5,5),\n",
    "            pool_k_size=(2,2),\n",
    "            pool_stride=2\n",
    "           ).to(device)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "results_dict = {}\n",
    "early_stopping = False\n",
    "\n",
    "# train\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "#     print('Epoch {}'.format(epoch), '-'*80)\n",
    "    results = {'train_loss': [], 'train_acc': []}\n",
    "    for idx, (feats_in, target_out) in enumerate(training_generator):\n",
    "        feats_in = feats_in.to(device).view(BATCH_SIZE, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/len(target_out)\n",
    "        loss = loss_function(tag_scores, target_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        results['train_loss'].append(loss.data.item())\n",
    "        results['train_acc'].append(accuracy)\n",
    "\n",
    "    if epoch%10==0:\n",
    "        print('Epoch {} | Training Loss: {}, Accuracy: {}%'.format(epoch, round(loss.data.item(),3), round(accuracy*100,3)))\n",
    "        if accuracy>0.99:\n",
    "            break\n",
    "\n",
    "    results_dict[epoch] = results\n",
    "    avg_acc = sum(results['train_acc'])/(idx+1)\n",
    "#     print('Average Accuracy: {}%'.format(round(avg_acc*100,3)))\n",
    "    \n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    print('-'*80)\n",
    "    for idx, (feats_in, target_out) in enumerate(testing_generator):\n",
    "        feats_in = feats_in.to(device).view(test_batch_size, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/test_batch_size\n",
    "        print('Testing Accuracy: {}%'.format(round(accuracy*100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 8.499, Accuracy: 10.0%\n",
      "Step 50 | Training Loss: 3.262, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: 3.227, Accuracy: 20.0%\n",
      "Step 150 | Training Loss: 2.929, Accuracy: 10.0%\n",
      "Step 200 | Training Loss: 2.837, Accuracy: 20.0%\n",
      "Step 250 | Training Loss: 3.35, Accuracy: 20.0%\n",
      "Average Accuracy: 22.584%\n",
      "Epoch 1 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.483, Accuracy: 90.0%\n",
      "Step 50 | Training Loss: 0.793, Accuracy: 70.0%\n",
      "Step 100 | Training Loss: 1.341, Accuracy: 70.0%\n",
      "Step 150 | Training Loss: 0.912, Accuracy: 60.0%\n",
      "Step 200 | Training Loss: 0.613, Accuracy: 90.0%\n",
      "Step 250 | Training Loss: 1.007, Accuracy: 50.0%\n",
      "Average Accuracy: 76.242%\n",
      "Epoch 2 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.059, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.219, Accuracy: 90.0%\n",
      "Step 100 | Training Loss: 0.755, Accuracy: 70.0%\n",
      "Step 150 | Training Loss: 0.043, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.092, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 1.228, Accuracy: 90.0%\n",
      "Average Accuracy: 89.966%\n",
      "Epoch 3 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.591, Accuracy: 80.0%\n",
      "Step 50 | Training Loss: 0.112, Accuracy: 90.0%\n",
      "Step 100 | Training Loss: 0.043, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 1.027, Accuracy: 70.0%\n",
      "Step 200 | Training Loss: 0.174, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.25, Accuracy: 90.0%\n",
      "Average Accuracy: 94.228%\n",
      "Epoch 4 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.007, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.017, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.022, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.138, Accuracy: 90.0%\n",
      "Step 200 | Training Loss: 0.033, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.076, Accuracy: 100.0%\n",
      "Average Accuracy: 97.483%\n",
      "Epoch 5 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.044, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.018, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.006, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.039, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.005, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.14, Accuracy: 90.0%\n",
      "Average Accuracy: 97.282%\n",
      "Epoch 6 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.003, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.01, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.254, Accuracy: 90.0%\n",
      "Average Accuracy: 97.987%\n",
      "Epoch 7 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.011, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 1.214, Accuracy: 60.0%\n",
      "Step 150 | Training Loss: 0.189, Accuracy: 90.0%\n",
      "Step 200 | Training Loss: 0.008, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.017, Accuracy: 100.0%\n",
      "Average Accuracy: 97.215%\n",
      "Epoch 8 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.168, Accuracy: 90.0%\n",
      "Step 50 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.745, Accuracy: 90.0%\n",
      "Step 150 | Training Loss: 0.005, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.027, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 97.047%\n",
      "Epoch 9 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.089, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.014, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 98.289%\n",
      "Epoch 10 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.003, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.592, Accuracy: 90.0%\n",
      "Step 200 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 98.993%\n",
      "Epoch 11 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 1.635, Accuracy: 90.0%\n",
      "Step 200 | Training Loss: 0.032, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 99.497%\n",
      "Epoch 12 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 99.966%\n",
      "Epoch 13 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 14 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "--------------------------------------------------------------------------------\n",
      "Testing Accuracy: 96.792%\n",
      "Epoch 0 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 5.488, Accuracy: 10.0%\n",
      "Step 50 | Training Loss: 3.256, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: 3.235, Accuracy: 10.0%\n",
      "Step 150 | Training Loss: 3.255, Accuracy: 0.0%\n",
      "Step 200 | Training Loss: 3.245, Accuracy: 20.0%\n",
      "Step 250 | Training Loss: 2.926, Accuracy: 10.0%\n",
      "Average Accuracy: 6.913%\n",
      "Epoch 1 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 2.125, Accuracy: 40.0%\n",
      "Step 50 | Training Loss: 2.514, Accuracy: 30.0%\n",
      "Step 100 | Training Loss: 1.741, Accuracy: 50.0%\n",
      "Step 150 | Training Loss: 0.377, Accuracy: 90.0%\n",
      "Step 200 | Training Loss: 0.145, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 1.037, Accuracy: 70.0%\n",
      "Average Accuracy: 54.899%\n",
      "Epoch 2 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.38, Accuracy: 90.0%\n",
      "Step 50 | Training Loss: 0.01, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.585, Accuracy: 70.0%\n",
      "Step 150 | Training Loss: 0.081, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.095, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.05, Accuracy: 100.0%\n",
      "Average Accuracy: 86.376%\n",
      "Epoch 3 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.073, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.055, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.174, Accuracy: 90.0%\n",
      "Step 150 | Training Loss: 0.079, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.332, Accuracy: 90.0%\n",
      "Step 250 | Training Loss: 0.381, Accuracy: 80.0%\n",
      "Average Accuracy: 94.966%\n",
      "Epoch 4 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.234, Accuracy: 90.0%\n",
      "Step 100 | Training Loss: 0.081, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.161, Accuracy: 90.0%\n",
      "Step 250 | Training Loss: 0.027, Accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 95.705%\n",
      "Epoch 5 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.09, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.082, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.068, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.019, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.015, Accuracy: 100.0%\n",
      "Average Accuracy: 98.054%\n",
      "Epoch 6 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.039, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.012, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.17, Accuracy: 90.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Average Accuracy: 99.329%\n",
      "Epoch 7 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.008, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 99.933%\n",
      "Epoch 8 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.006, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 9 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.004, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 10 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 11 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 12 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 13 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 14 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "--------------------------------------------------------------------------------\n",
      "Testing Accuracy: 97.731%\n",
      "Epoch 0 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 8.498, Accuracy: 10.0%\n",
      "Step 50 | Training Loss: 3.283, Accuracy: 0.0%\n",
      "Step 100 | Training Loss: 3.268, Accuracy: 0.0%\n",
      "Step 150 | Training Loss: 3.0, Accuracy: 10.0%\n",
      "Step 200 | Training Loss: 1.369, Accuracy: 80.0%\n",
      "Step 250 | Training Loss: 1.928, Accuracy: 70.0%\n",
      "Average Accuracy: 31.007%\n",
      "Epoch 1 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.589, Accuracy: 90.0%\n",
      "Step 50 | Training Loss: 0.568, Accuracy: 80.0%\n",
      "Step 100 | Training Loss: 0.975, Accuracy: 80.0%\n",
      "Step 150 | Training Loss: 1.342, Accuracy: 80.0%\n",
      "Step 200 | Training Loss: 0.489, Accuracy: 80.0%\n",
      "Step 250 | Training Loss: 1.133, Accuracy: 70.0%\n",
      "Average Accuracy: 84.295%\n",
      "Epoch 2 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.085, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.104, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.052, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.015, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.319, Accuracy: 80.0%\n",
      "Step 250 | Training Loss: 0.055, Accuracy: 100.0%\n",
      "Average Accuracy: 91.51%\n",
      "Epoch 3 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.051, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.161, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.06, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.306, Accuracy: 90.0%\n",
      "Step 250 | Training Loss: 0.003, Accuracy: 100.0%\n",
      "Average Accuracy: 97.651%\n",
      "Epoch 4 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.005, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.095, Accuracy: 90.0%\n",
      "Step 100 | Training Loss: 0.033, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.004, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.082, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.006, Accuracy: 100.0%\n",
      "Average Accuracy: 98.423%\n",
      "Epoch 5 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.015, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 99.43%\n",
      "Epoch 6 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.018, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.036, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 98.356%\n",
      "Epoch 7 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.004, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.209, Accuracy: 90.0%\n",
      "Step 100 | Training Loss: 0.003, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.004, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.004, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.007, Accuracy: 100.0%\n",
      "Average Accuracy: 98.154%\n",
      "Epoch 8 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.012, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.012, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.021, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.006, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.015, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.011, Accuracy: 100.0%\n",
      "Average Accuracy: 97.282%\n",
      "Epoch 9 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.002, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 99.698%\n",
      "Epoch 10 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 11 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 12 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 13 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "Epoch 14 --------------------------------------------------------------------------------\n",
      "Step 0 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 50 | Training Loss: 0.001, Accuracy: 100.0%\n",
      "Step 100 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 150 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 200 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Step 250 | Training Loss: 0.0, Accuracy: 100.0%\n",
      "Average Accuracy: 100.0%\n",
      "--------------------------------------------------------------------------------\n",
      "Testing Accuracy: 97.653%\n"
     ]
    }
   ],
   "source": [
    "# 3. Comments on reproducibility of accuracies: Apply gradient clipping\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "\n",
    "# revert values\n",
    "BATCH_SIZE=10\n",
    "NUM_EPOCHS=15\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim, kernel_size, pool_k_size, pool_stride):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(embed_dim, hidden_dim1, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim1, hidden_dim2, kernel_size=kernel_size)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_k_size, stride=pool_stride)\n",
    "        self.fc1 = nn.Linear(hidden_dim2*5*5, hidden_dim3)\n",
    "        self.fc2 = nn.Linear(hidden_dim3, output_dim)\n",
    "        \n",
    "    def forward(self, feats_in):\n",
    "        batch_size, e_, e_, s_ = feats_in.shape\n",
    "        x = self.pool(F.relu(self.conv1(feats_in.float())))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x),dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# load data\n",
    "training_set = FormatDataset(train_data, np.asarray(train_labels_num))\n",
    "training_generator = DataLoader(training_set, batch_size=BATCH_SIZE, **GENERATOR_PARAMS)\n",
    "\n",
    "test_batch_size = test_data.shape[0]\n",
    "test_set = FormatDataset(test_data, np.asarray(test_labels_num))\n",
    "testing_generator = DataLoader(test_set, batch_size=test_batch_size, **GENERATOR_PARAMS)\n",
    "\n",
    "########### RUN 1 #############\n",
    "# load model\n",
    "model = CNN(embed_dim=1,\n",
    "            hidden_dim1=20, \n",
    "            hidden_dim2=50,\n",
    "            hidden_dim3=500,\n",
    "            output_dim=26, \n",
    "            kernel_size=(5,5),\n",
    "            pool_k_size=(2,2),\n",
    "            pool_stride=2\n",
    "           ).to(device)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "results_dict = {}\n",
    "early_stopping = False\n",
    "\n",
    "# train\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Epoch {}'.format(epoch), '-'*80)\n",
    "    results = {'train_loss': [], 'train_acc': []}\n",
    "    for idx, (feats_in, target_out) in enumerate(training_generator):\n",
    "        feats_in = feats_in.to(device).view(BATCH_SIZE, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/len(target_out)\n",
    "        loss = loss_function(tag_scores, target_out)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        results['train_loss'].append(loss.data.item())\n",
    "        results['train_acc'].append(accuracy)\n",
    "\n",
    "        if idx%PRINT_ROUND==0:\n",
    "            print('Step {} | Training Loss: {}, Accuracy: {}%'.format(idx, round(loss.data.item(),3), round(accuracy*100,3)))\n",
    "\n",
    "    results_dict[epoch] = results\n",
    "    avg_acc = sum(results['train_acc'])/(idx+1)\n",
    "    print('Average Accuracy: {}%'.format(round(avg_acc*100,3)))\n",
    "    \n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    print('-'*80)\n",
    "    for idx, (feats_in, target_out) in enumerate(testing_generator):\n",
    "        feats_in = feats_in.to(device).view(test_batch_size, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/test_batch_size\n",
    "        print('Testing Accuracy: {}%'.format(round(accuracy*100,3)))\n",
    "        \n",
    "########### RUN 2 #############\n",
    "# load model\n",
    "model = CNN(embed_dim=1,\n",
    "            hidden_dim1=20, \n",
    "            hidden_dim2=50,\n",
    "            hidden_dim3=500,\n",
    "            output_dim=26, \n",
    "            kernel_size=(5,5),\n",
    "            pool_k_size=(2,2),\n",
    "            pool_stride=2\n",
    "           ).to(device)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "results_dict = {}\n",
    "early_stopping = False\n",
    "\n",
    "# train\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Epoch {}'.format(epoch), '-'*80)\n",
    "    results = {'train_loss': [], 'train_acc': []}\n",
    "    for idx, (feats_in, target_out) in enumerate(training_generator):\n",
    "        feats_in = feats_in.to(device).view(BATCH_SIZE, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/len(target_out)\n",
    "        loss = loss_function(tag_scores, target_out)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        results['train_loss'].append(loss.data.item())\n",
    "        results['train_acc'].append(accuracy)\n",
    "\n",
    "        if idx%PRINT_ROUND==0:\n",
    "            print('Step {} | Training Loss: {}, Accuracy: {}%'.format(idx, round(loss.data.item(),3), round(accuracy*100,3)))\n",
    "\n",
    "    results_dict[epoch] = results\n",
    "    avg_acc = sum(results['train_acc'])/(idx+1)\n",
    "    print('Average Accuracy: {}%'.format(round(avg_acc*100,3)))\n",
    "    \n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    print('-'*80)\n",
    "    for idx, (feats_in, target_out) in enumerate(testing_generator):\n",
    "        feats_in = feats_in.to(device).view(test_batch_size, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/test_batch_size\n",
    "        print('Testing Accuracy: {}%'.format(round(accuracy*100,3)))\n",
    "        \n",
    "########### RUN 3 #############\n",
    "# load model\n",
    "model = CNN(embed_dim=1,\n",
    "            hidden_dim1=20, \n",
    "            hidden_dim2=50,\n",
    "            hidden_dim3=500,\n",
    "            output_dim=26, \n",
    "            kernel_size=(5,5),\n",
    "            pool_k_size=(2,2),\n",
    "            pool_stride=2\n",
    "           ).to(device)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "results_dict = {}\n",
    "early_stopping = False\n",
    "\n",
    "# train\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Epoch {}'.format(epoch), '-'*80)\n",
    "    results = {'train_loss': [], 'train_acc': []}\n",
    "    for idx, (feats_in, target_out) in enumerate(training_generator):\n",
    "        feats_in = feats_in.to(device).view(BATCH_SIZE, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/len(target_out)\n",
    "        loss = loss_function(tag_scores, target_out)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        results['train_loss'].append(loss.data.item())\n",
    "        results['train_acc'].append(accuracy)\n",
    "\n",
    "        if idx%PRINT_ROUND==0:\n",
    "            print('Step {} | Training Loss: {}, Accuracy: {}%'.format(idx, round(loss.data.item(),3), round(accuracy*100,3)))\n",
    "\n",
    "    results_dict[epoch] = results\n",
    "    avg_acc = sum(results['train_acc'])/(idx+1)\n",
    "    print('Average Accuracy: {}%'.format(round(avg_acc*100,3)))\n",
    "    \n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    print('-'*80)\n",
    "    for idx, (feats_in, target_out) in enumerate(testing_generator):\n",
    "        feats_in = feats_in.to(device).view(test_batch_size, 32, 32).unsqueeze(1)\n",
    "        target_out = target_out.to(device)\n",
    "        tag_scores = model(feats_in).to(device)\n",
    "        predicted = torch.argmax(tag_scores, dim=1).detach()\n",
    "        correct = sum([1 for pred,act in zip(predicted,target_out) if pred==act])\n",
    "        accuracy = correct/test_batch_size\n",
    "        print('Testing Accuracy: {}%'.format(round(accuracy*100,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
